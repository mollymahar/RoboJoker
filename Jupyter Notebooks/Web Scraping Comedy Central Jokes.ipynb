{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_page(content):\n",
    "    ''' Remove extra spaces between HTML tags. '''\n",
    "    content = ''.join([line.strip() for line in content.split('\\n')])\n",
    "    return content\n",
    "\n",
    "def find_all_topic_page_links():\n",
    "    topic_page_urls = []\n",
    "    try:\n",
    "        response = urllib.request.urlopen('http://jokes.cc.com/joke-categories')\n",
    "        content = response.read().decode('utf8')\n",
    "        content = preprocess_page(content) # Now *content* is a string containing the html page, ready for processing with BeautifulSoup\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        for category in soup.find(\"ul\", class_='list_horiz'):\n",
    "            for link in category.find_all('a'):\n",
    "                topic_page_urls.append(link.get('href'))\n",
    "    except IOError:\n",
    "        print(\"Unable to open URL\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        return topic_page_urls\n",
    "\n",
    "def find_joke_page_links_from_topic_pages(url_list):\n",
    "    '''Find and return all the individual joke page links from the current topic page.'''\n",
    "    joke_page_urls = []\n",
    "    try:\n",
    "        for link in url_list:\n",
    "            topic_category = ''\n",
    "            response = urllib.request.urlopen(link)\n",
    "            content = response.read().decode('utf8')\n",
    "            content = preprocess_page(content) \n",
    "            # Now *content* is a string containing the html page, ready for processing with BeautifulSoup\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "            for each in soup.find(\"div\", class_='module_content'):\n",
    "                topic_category = each.find('span', class_='bgb').get_text() \n",
    "                # this pulls in the greater topic or category of joke, to be saved with the link\n",
    "                for item in each.find('ul'):\n",
    "                    for link in item.find_all('a'):\n",
    "                        joke_page_urls.append([topic_category, link.get('href')])\n",
    "    except IOError:\n",
    "        print(\"Unable to open URL\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        return joke_page_urls\n",
    "\n",
    "def soupify_jokes(url_list):\n",
    "    '''Reads in the list of topics and URLs, visits each one, and then saves each joke \n",
    "    (and topic) to the list of jokes.'''\n",
    "    jokes = []\n",
    "    try:\n",
    "        for idx in range(0,len(url_list)):\n",
    "            joke = \"\"\n",
    "            response = urllib.request.urlopen(url_list[idx][1])\n",
    "            content = response.read().decode('utf8')\n",
    "            content = preprocess_page(content) \n",
    "            # Now *content* is a string containing the html page, ready for processing with BeautifulSoup\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "            for each in soup.find(\"div\", class_='content_wrap'):\n",
    "                if each.name == 'p':\n",
    "                    for br in each.find_all(\"br\"):\n",
    "                        br.replace_with(\"\\n\")\n",
    "                    joke += each.get_text() + \" \"\n",
    "            jokes.append([url_list[idx][0],url_list[idx][1], joke])          \n",
    "    except IOError:\n",
    "        print(\"Unable to open URL\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        return jokes\n",
    "    \n",
    "def soupify_jokes_from_csv(filename):\n",
    "    '''Reads in the list of topics/categories and URLs from a csv, visits each one, and then saves each joke \n",
    "    (and topic) to the list of jokes.'''\n",
    "    # start by importing the csv and saving those to a list (so I don't have to re-scrap those URLs each time)\n",
    "    url_list = []\n",
    "    with open(filename) as f:\n",
    "        for each in f.readlines():\n",
    "            url_list.append(each.strip().split(','))\n",
    "    # then move onto processing those\n",
    "    jokes = []\n",
    "    try:\n",
    "        for idx in range(11412, len(url_list)):\n",
    "#         for idx in range(0,len(url_list)):\n",
    "            joke = \"\"\n",
    "            # going to try to use the Google cached version so I don't upset/inconvenience the Comedy Central site\n",
    "#             cached_url = 'http://webcache.googleusercontent.com/search?q=cache:'+ url_list[idx][1]\n",
    "#             response = urllib.request.urlopen(cached_url)\n",
    "            req = urllib.request.Request(url_list[idx][1], data=None, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})\n",
    "            response = urllib.request.urlopen(req)\n",
    "            content = response.read().decode('utf8')\n",
    "            content = preprocess_page(content)\n",
    "            # Now *content* is a string containing the html page, ready for processing with BeautifulSoup\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "            for each in soup.find(\"div\", class_='content_wrap'):\n",
    "                if each.name == 'p':\n",
    "                    for br in each.find_all(\"br\"):\n",
    "                        br.replace_with(\"\\n \")\n",
    "                    joke += each.get_text() + \" \"\n",
    "            jokes.append([url_list[idx][0],url_list[idx][1], joke])          \n",
    "    except IOError:\n",
    "        print(\"Unable to open URL: \" + url_list[idx][1])\n",
    "        return jokes\n",
    "        exit(1)\n",
    "    else:\n",
    "        return jokes\n",
    "    \n",
    "def write_to_file(jokes_list, filename='all_cc_jokes.csv'):\n",
    "    final_result = pd.DataFrame(jokes_list)\n",
    "    final_result.columns = ['Category', 'URL', 'Joke']\n",
    "    final_result.to_csv(filename, sep=',', header=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# commenting this out now that I already ran it and saved out the URL file (so I don't have to scrape it again)\n",
    "\n",
    "# topic_page_urls = find_all_topic_page_links()\n",
    "# print(len(topic_page_urls))\n",
    "# output = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# commenting this out now that I already ran it and saved out the URL file (so I don't have to scrape it again)\n",
    "\n",
    "# joke_page_urls = find_joke_page_links_from_topic_pages(topic_page_urls)\n",
    "\n",
    "# print(len(joke_page_urls))\n",
    "# output = 14553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# commenting this out now that I already ran it and saved out the URL file (so I don't have to scrape it again)\n",
    "\n",
    "# jokes_df = pd.DataFrame(joke_page_urls)\n",
    "# jokes_df.to_csv('joke_urls.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "# did this for first batch, just modified the range of the soupify function.\n",
    "\n",
    "write_to_file(soupify_jokes_from_csv('joke_urls.csv'))\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open URL: http://jokes.cc.com/funny-pop-culture---celebrity/tsud16/jeff-cesario--greatest-democracy-in-the-world\n"
     ]
    }
   ],
   "source": [
    "# did this for the second batch, again modifying the range for the soupify function\n",
    "write_to_file(soupify_jokes_from_csv('joke_urls.csv'))\n",
    "# starting this call at 7:58pm on Friday, finished at 1:18am Saturday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# did this for the third batch, again, modifying the range for the soupify function\n",
    "write_to_file(soupify_jokes_from_csv('joke_urls.csv'))\n",
    "# starting this call at 7:01am on Saturday, ended at 8:35am Saturday\n",
    "# this file got overwritten when I was saving out the whole long list of jokes...hopefully I don't need it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seem to be missing from http://jokes.cc.com/funny-miscellaneous/wfnyak/jeff-caldwell--birth-control\n",
    "to http://jokes.cc.com/funny-pick-up-lines/c4uzmn/zombie-booty-call----catch\n",
    "\n",
    "* Not sure if this is actually happening, some jokes are cross-listed and appear with multiple URLs based on the different topics that they could fit into. When that happens, they all share the same final URL (after last slash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_jokes_back_in(filename):\n",
    "    df1 = pd.read_csv(filename, \n",
    "                     sep = ',', \n",
    "                     skiprows = 1, \n",
    "                     names = ['Index', 'Category', 'URL', 'Joke'])\n",
    "    return df1\n",
    "\n",
    "jokes_1 = read_jokes_back_in('jokes_result.csv')\n",
    "jokes_2 = read_jokes_back_in('jokes_result_2.csv')\n",
    "jokes_3 = read_jokes_back_in('jokes_result_3.csv')\n",
    "\n",
    "# all_jokes = []\n",
    "# with open('jokes_result.csv') as f1:\n",
    "#     jokes_1 = f1.readlines()\n",
    "# for each in jokes_1[:5]:\n",
    "#     all_jokes.append(each.split(','))\n",
    "# for each in all_jokes:\n",
    "#     print(each)\n",
    "# with open('jokes_result_2.csv') as f2:\n",
    "#     jokes_2 = f2.readlines.strip().split(',')\n",
    "# with open('jokes_result_3.csv') as f3:\n",
    "#     jokes_3 = f3.readlines.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15054\n"
     ]
    }
   ],
   "source": [
    "print(len(jokes_1) + len(jokes_2) + len(jokes_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dropping the index column because it had reset with each new file I started\n",
    "jokes_1.drop('Index', axis=1, inplace=True)\n",
    "jokes_2.drop('Index', axis=1, inplace=True)\n",
    "jokes_3.drop('Index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_frames = [jokes_1, jokes_2, jokes_3]\n",
    "all_jokes = pd.concat(all_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category                                             Yo' Mama\n",
       "URL         http://jokes.cc.com/funny-yo--mama/rx4x0c/yo--...\n",
       "Joke        Yo' sister is so ugly, I thought she was Yo' M...\n",
       "Name: 15050, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_jokes.loc[15050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(all_jokes)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
