{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from doc [gensim doc2vec & IMDB sentiment dataset](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch and prep exactly as in Mikolov's go.sh shell script. (Note this cell tests for existence of required files, so steps won't repeat once the final summary file (`aclImdb/alldata-id.txt`) is available alongside this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.data import find\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import random\n",
    "\n",
    "data = pd.read_csv('../nodups_combined_jokes.csv', sep = ',', index_col = 0)\n",
    "# word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "# model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "# tfidfvectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "# tfidf_counts = tfidfvectorizer.fit_transform(data['text'].values.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os.path\n",
    "# assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is small enough to be read into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9076 docs: 0 train-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "alldocs = []\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split')\n",
    "for line_no, line in enumerate(data['text']):\n",
    "# for line_no, line in enumerate(data2['text']):\n",
    "    tokens = gensim.utils.to_unicode(normalize_text(line)).split()\n",
    "    words = tokens\n",
    "    tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "#     split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "    split = ['train']\n",
    "    alldocs.append(SentimentDocument(words, tags, split))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "# test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment' % (len(doc_list), len(train_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
    "\n",
    "Parameter choices below vary:\n",
    "\n",
    "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
    "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d200,n5,w5,mc2,t8)\n",
      "Doc2Vec(dbow,d200,n5,mc2,t8)\n",
      "Doc2Vec(dm/m,d200,n5,w10,mc2,t8)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=200, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=200, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=200, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper methods for evaluating error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) – with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
    "\n",
    "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2016-12-06 18:52:31.699664\n",
      "completed pass 1 at alpha 0.025000\n",
      "completed pass 2 at alpha 0.024200\n",
      "completed pass 3 at alpha 0.023400\n",
      "completed pass 4 at alpha 0.022600\n",
      "completed pass 5 at alpha 0.021800\n",
      "completed pass 6 at alpha 0.021000\n",
      "completed pass 7 at alpha 0.020200\n",
      "completed pass 8 at alpha 0.019400\n",
      "completed pass 9 at alpha 0.018600\n",
      "completed pass 10 at alpha 0.017800\n",
      "completed pass 11 at alpha 0.017000\n",
      "completed pass 12 at alpha 0.016200\n",
      "completed pass 13 at alpha 0.015400\n",
      "completed pass 14 at alpha 0.014600\n",
      "completed pass 15 at alpha 0.013800\n",
      "completed pass 16 at alpha 0.013000\n",
      "completed pass 17 at alpha 0.012200\n",
      "completed pass 18 at alpha 0.011400\n",
      "completed pass 19 at alpha 0.010600\n",
      "completed pass 20 at alpha 0.009800\n",
      "completed pass 21 at alpha 0.009000\n",
      "completed pass 22 at alpha 0.008200\n",
      "completed pass 23 at alpha 0.007400\n",
      "completed pass 24 at alpha 0.006600\n",
      "completed pass 25 at alpha 0.005800\n",
      "completed pass 26 at alpha 0.005000\n",
      "completed pass 27 at alpha 0.004200\n",
      "completed pass 28 at alpha 0.003400\n",
      "completed pass 29 at alpha 0.002600\n",
      "completed pass 30 at alpha 0.001800\n",
      "END 2016-12-06 18:57:43.809102\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 30)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list)\n",
    "            duration = '%.1f' % elapsed()\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print best error rates achieved\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (1562): «i hope you know cpr because you're taking my breath away .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d200,n5,mc2,t8):\n",
      "\n",
      "MOST (8987, 0.6442590951919556): «you better hope you marry rich .»\n",
      "\n",
      "MEDIAN (4486, 0.27110743522644043): «i saw a transvestite holding one of those fancy handbags that said 'guess . ' i said , 'you're a man ? '»\n",
      "\n",
      "LEAST (2515, -0.019454311579465866): «an army major visits the sick soldiers , goes up to one private and asks :»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "# 0 for dm/c, 1 dbow, 2 dm/m\n",
    "model = simple_models[1]\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 2), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: «Yo mama's like a brick. dirty, flat on both sides, and always getting laid by Mexicans.»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d200,n5,mc2,t8):\n",
      "\n",
      "MOST (3778, 0.5133495926856995): «yo mama's so fat , she uses a semi-trailer as a couch .»\n",
      "\n",
      "MEDIAN (1571, 0.196953684091568): «you're like a dictionary -- you add meaning to my life .»\n",
      "\n",
      "LEAST (4585, -0.04875617474317551): «an old woman says to an old man at the rest home , \" i can guess your age . \" the man doesn't believe her , but tells her to go ahead and try . \" pull down your pants , \" she says . she inspects his rear end for a few minutes and then says , \" you're 84 years old . \" \" that's amazing , \" the man says . \" how did you know ? \" \" you told me yesterday . \"»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = (\"Yo mama's like a brick. dirty, flat on both sides, and always getting laid by Mexicans.\")\n",
    "tokens = gensim.utils.to_unicode(normalize_text(sample_text)).split()\n",
    "vector = model.infer_vector(tokens)\n",
    "# print(dir(model))\n",
    "sims = model.docvecs.most_similar([vector], topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET: «%s»\\n' % (sample_text))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 2), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../doc2vecmodel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save docvecs\n",
    "model = simple_models[1]\n",
    "docvecs = model.docvecs\n",
    "# dir(docvecs)\n",
    "# docvecs.__dict__\n",
    "\n",
    "# np.save('docvec.npy', docvecs.doctag_syn0norm)\n",
    "np.save('../docvec_unique.npy', docvecs.doctag_syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 9076,\n",
       " 'doctag_syn0': array([[ 0.41157216, -0.15710752,  0.21310677, ...,  0.01677105,\n",
       "          0.28926131, -0.65804493],\n",
       "        [ 0.3731648 , -0.45764539,  0.15154262, ...,  0.20910114,\n",
       "         -0.09889825, -0.91377634],\n",
       "        [ 0.12960051,  0.08168242, -0.12865862, ...,  0.12778267,\n",
       "         -0.01339548, -0.26391193],\n",
       "        ..., \n",
       "        [ 0.011427  , -0.03433529,  0.17223276, ...,  0.11492898,\n",
       "          0.38475436, -0.31939203],\n",
       "        [-0.1054318 ,  0.4818863 , -0.11470713, ..., -0.29730418,\n",
       "         -0.27723277, -0.20750199],\n",
       "        [ 0.25216168,  0.27897939, -0.15056613, ...,  0.06536528,\n",
       "          0.14018208, -0.12054388]], dtype=float32),\n",
       " 'doctag_syn0_lockf': array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32),\n",
       " 'doctag_syn0norm': None,\n",
       " 'doctags': {},\n",
       " 'mapfile_path': None,\n",
       " 'max_rawint': -1,\n",
       " 'offset2doctag': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvecs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Structure stuff\n",
    "def joke_structure(joke):\n",
    "    '''This function takes in a joke as a single string. Then it calculates the length of the joke (in words),\n",
    "    the number of sentences in the joke, and whether (1) or not (0) the joke involves a question.'''\n",
    "    # one additional thought is to catch whether it is a joke AT somebody (uses \"you\") or if it's a general, \n",
    "    # situational joke that is always in the third person.\n",
    "    structure = []\n",
    "    \n",
    "    # first, calculate the length of the joke (I'll define this in words):\n",
    "    structure.append(count_words(joke))\n",
    "    \n",
    "    # then, calculate the number of sentences/segments in the joke:\n",
    "    structure.append(count_sents(joke))\n",
    "    \n",
    "    # finally, determine whether or not the joke involves a question:\n",
    "    structure.append(is_a_question(joke))\n",
    "    \n",
    "    return structure\n",
    "    \n",
    "def count_words(joke):\n",
    "    words = nltk.word_tokenize(joke)\n",
    "    return len(words)\n",
    "    \n",
    "def count_sents(joke):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sents = sent_tokenizer.tokenize(joke) # Split text into sentences    \n",
    "    return len(sents)\n",
    "\n",
    "def is_a_question(joke):\n",
    "    words = nltk.word_tokenize(joke)\n",
    "    if \"?\" in words:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append these features to an existing array\n",
    "data = pd.read_csv('../combined_jokes_unique.csv', sep = ',', index_col = 0)\n",
    "new_features = np.zeros((len(data),3))\n",
    "for i, joke in enumerate(data):\n",
    "    try:\n",
    "        new_features[i] = np.asarray(joke_structure(joke))\n",
    "    except:\n",
    "        pass\n",
    "existing_features = np.load('../docvec_unique.npy')\n",
    "print(existing_features.shape)\n",
    "print(new_features.shape)\n",
    "combined_features = np.append(existing_features, new_features, axis=1)\n",
    "np.save('../combined_features_unique.npy', combined_features)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
