{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.data import find\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import random\n",
    "\n",
    "data = pd.read_csv('./all_cc_jokes.csv', sep = ',', index_col = 0, names = ['type', 'link', 'text'])\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "tfidfvectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "tfidf_counts = tfidfvectorizer.fit_transform(data['text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./all_cc_jokes.csv', sep = ',', index_col = 0, names = ['type', 'link', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "doing joke 20"
     ]
    }
   ],
   "source": [
    "def latent_topic(joke_index, tfidf_counts, tfidfvectorizer, model):\n",
    "    \"\"\"creates a latent topic for a joke using an existing word2vec model\n",
    "    does so by doing a weighted avg of word2vec \"\"\"\n",
    "    print('\\rdoing joke {}'.format(joke_index), end='')\n",
    "    scores = tfidf_counts[joke_index]\n",
    "    result = None\n",
    "    for j, index in enumerate(scores.indices):\n",
    "        curr_word = tfidfvectorizer.get_feature_names()[index]\n",
    "        try:\n",
    "            if result is None:\n",
    "                result = model[curr_word]\n",
    "            else:\n",
    "                result += model[curr_word]\n",
    "        except:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "def latent_topic3(joke_index, tfidf_counts, tfidfvectorizer, model):\n",
    "    \"\"\"creates a latent topic for a joke using an existing word2vec model\n",
    "    does so by doing a weighted avg of word2vec \"\"\"\n",
    "    print('\\rdoing joke {}'.format(joke_index), end='')\n",
    "    scores = tfidf_counts[joke_index]\n",
    "    result = None\n",
    "    for j, index in enumerate(scores.indices):\n",
    "        curr_word = tfidfvectorizer.get_feature_names()[index]\n",
    "        try:\n",
    "            if result is None:\n",
    "                result = model[curr_word] * scores.data[j]\n",
    "            else:\n",
    "                result += model[curr_word] *scores.data[j]\n",
    "        except:\n",
    "            pass\n",
    "    if result is not None:\n",
    "        return result/len(scores.indices)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def latent_topic2(joke_index, tfidf_counts, tfidfvectorizer, model):\n",
    "    \"\"\"creates a latent topic for a joke using an existing word2vec model\n",
    "    does so by doing a weighted avg of word2vec \"\"\"\n",
    "    print('\\rdoing joke {}'.format(joke_index), end='')\n",
    "    scores = tfidf_counts[joke_index]\n",
    "    result = None\n",
    "    for index in scores.indices:\n",
    "        curr_word = tfidfvectorizer.get_feature_names()[index]\n",
    "        try:\n",
    "            if result is None:\n",
    "                result = model[curr_word].reshape((1,300))\n",
    "            else:\n",
    "                result = np.append(result,model[curr_word].reshape((1,300)), axis=0)\n",
    "        except:\n",
    "            pass\n",
    "    if result is None:\n",
    "        return None\n",
    "    print(result.shape)\n",
    "    mean = np.mean(result, axis=0)\n",
    "    std = np.std(result, axis=0)\n",
    "    return np.append(mean-std, mean+std, axis = 0).reshape((600))\n",
    "result = latent_topic(20, tfidf_counts, tfidfvectorizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded latent topics for jokes from file\n"
     ]
    }
   ],
   "source": [
    "generate = False\n",
    "try:\n",
    "    all_latent_topics = np.load('./docvec.npy')\n",
    "    print(\"successfully loaded latent topics for jokes from file\")\n",
    "except:\n",
    "    generate = True\n",
    "\n",
    "if generate:\n",
    "    print(\"generating latent topics for jokes file,\")\n",
    "    all_latent_topics = np.zeros((data.shape[0],300))\n",
    "    for i in range(data.shape[0]):\n",
    "        result = latent_topic(i, tfidf_counts, tfidfvectorizer, model)\n",
    "        if result is not None:\n",
    "            all_latent_topics[i] = result \n",
    "    print(\"\\ndone! saved as latent_topics.npy\")\n",
    "    np.save('./latent_topics.npy', all_latent_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11882\n",
      "That sport jacket totally matches your sneakers.\r\n",
      "  \n",
      "how many equal 0 1\n",
      "ssd 0.188471\n",
      "2151\n",
      "Yes, that's a pistol drill in my pants, and yes, I'm glad to see you.\r\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(index, latent_topics, jokes):\n",
    "    curr_topic = latent_topics[index]\n",
    "    ssd = np.sum(np.square(latent_topics - curr_topic), axis=1)\n",
    "    top = 0\n",
    "    index = np.argsort(ssd)[top]\n",
    "    print(\"how many equal 0\", np.sum(ssd == 0))\n",
    "\n",
    "    while ssd[index] == 0 and top < 8:\n",
    "#         print (\"skip, identical\")\n",
    "#         print ('index', index)\n",
    "#         print ('actual', np.sum(np.square(latent_topics[index] - curr_topic)))\n",
    "#         print (latent_topics[index][:10])\n",
    "#         print(curr_topic[:10])\n",
    "#         print ('ssd', ssd[index])\n",
    "#         print (jokes[index])\n",
    "        top += 1\n",
    "        index = np.argsort(ssd)[top]\n",
    "\n",
    "#     print (np.sum(np.square(latent_topics[np.argsort(ssd)[index]] - curr_topic)))\n",
    "#     print(np.sum(np.square(latent_topics - curr_topic), axis=1)[np.argsort(ssd)[index]])\n",
    "#     print (ssd[np.argsort(ssd)[index]])\n",
    "#     print (ssd[np.argsort(ssd)[index+1]])\n",
    "#     print (ssd[np.argsort(ssd)[index-1]])\n",
    "    print ('ssd', ssd[index])\n",
    "    return np.argsort(ssd)[index]\n",
    "\n",
    "# set to random, or you can set index to a number\n",
    "index=random.randint(0,data.shape[0])\n",
    "# index=13929\n",
    "print(index)\n",
    "print (data['text'][index])\n",
    "similar_index = find_most_similar(index, all_latent_topics, data['text'])\n",
    "print(similar_index)\n",
    "print (data['text'][similar_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  8129\n",
      "rating:  6.78795546031\n",
      "On a beautiful deserted island in the middle of nowhere, the following people are stranded:   2 Italian men and 1 Italian woman, 2 French men and 1 French woman, 2 German men and 1 German woman, 2 Greek men and 1 Greek woman, 2 English men and 1 English woman, 2 Polish men and 1 Polish woman, 2 Japanese men and 1 Japanese woman, 2 Australian men and 1 Australian woman, 2 New Zealander men and 1 New Zealander woman, 2 Irish men and 1 Irish woman, 2 American men and 1 American woman.   One month later, the following things have occurred....\r\n",
      " One Italian man killed the other Italian man for the Italian woman.  \r\n",
      " The two French men and the French woman are living happily together, having loads of sex. \r\n",
      " The two German men have a strict weekly schedule of when they alternate with the German woman.  \r\n",
      " The two Greek men are sleeping with each other and the Greek woman is cleaning and cooking for them.  \r\n",
      " The two English men are waiting for someone to introduce them to the English woman. \r\n",
      " The Polish men took a long look at the endless ocean and one look at the Polish woman and they started swimming.  \r\n",
      " The two Japanese men have faxed Tokyo and are waiting for instructions.  \r\n",
      " The two Australian men beat each other senseless fighting  over the Australian woman, who called them both bloody wankers and is checking out all the other men.\r\n",
      " Both New Zealand men are searching the island for sheep. \r\n",
      " The Irish began by dividing the island into North and South  and setting up a distillery. They do not remember if sex is in the picture because it gets sort of foggy after the first few liters of coconut whiskey, but they are satisfied in that at least the English are not getting any.    \r\n",
      " The American woman keeps on bitching about her body being her own, the true nature of feminism, how she can do everything that they can do, about the necessity  of fulfillment, the equal division of household chores, how her last boyfriend respected her opinions and treated her much nicer and how her relationship with her mother is mproving. The two American men have committed suicide. \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import linear_model\n",
    "\n",
    "def guess_ratings(indices, ratings, joke_features):\n",
    "    alpha = 9999999999\n",
    "#     lasso =linear_model.LinearRegression()\n",
    "#     lasso = Lasso(alpha=alpha, max_iter = 20000, tol=.1)\n",
    "    lasso = linear_model.Ridge (alpha = .1)\n",
    "#     lasso = ElasticNet(alpha=alpha, l1_ratio=0.7, max_iter=20000)\n",
    "#     lasso\n",
    "\n",
    "    y_pred_lasso = lasso.fit(joke_features[indices], ratings).predict(joke_features)\n",
    "    y_pred_lasso -= np.mean(y_pred_lasso)\n",
    "    y_pred_lasso *= 1.5/np.std(y_pred_lasso)\n",
    "    y_pred_lasso += 2.5\n",
    "    return y_pred_lasso\n",
    "# add a joke's index and then its corresponding rating \n",
    "indices = [9539, 9943, 14327, 14328, 13048, 13058, 8698, 8701, 2578, 2598, 5497, 6235, 13886, 0, 15017, 13882, 10817, 13867, 14196, 13860, 13857, 10764, 13876, 10830, 10820, 13894, 13889, 13864, 13893, 14086, 13871, 13874, 12070, 13861, 13829, 13871, 12806, 12916, 14609, 14506, 14232, 13892, 13896, 13052, 13078, 4479, 10665, 3220, 5178, 4971, 10065, 14513, 4851, 14453, 14406, 14427, 221, 1399, 9912, 294, 9192, 130, 9960, 5156, 14276, 10745, 10175, 14434]\n",
    "ratings = [4.5, 2, 3.3, 4, 5, 2, 3.8, 3, 2, 4, 3.5, 1, 3, 4, 3.5, 3, 0, 1, .5, 0, 2, 1, 0, 4, 4.5, 4.5, 2, 3, 3, 1, 2, 4, 1, 4, 2, 3, 4, 0, 4, 1, 2.1, 1, 2, 0, 0, 0, 5, 3, 4.5, 5, 1, 3.3, 3.4, 3, 3, 1, 0, 0, 0, 3, 2, 3, 1, 3, 2, 2, 2, 5]\n",
    "indices, ratings = np.asarray(indices), np.asarray(ratings)\n",
    "guesses = guess_ratings(indices, ratings, all_latent_topics)\n",
    "\n",
    "# uncomment this to get \"good jokes\"\n",
    "blah = random.randint(-100, -1)\n",
    "\n",
    "# uncomment this to get \"bad jokes\"\n",
    "# blah = random.randint(0, 1000)\n",
    "\n",
    "# uncomment this to get a completely random joke\n",
    "# blah = random.randint(0, 15000)\n",
    "\n",
    "\n",
    "joke_index = np.argsort(guesses)[blah]\n",
    "while joke_index in indices:\n",
    "    blah -= 1\n",
    "    joke_index = np.argsort(guesses)[blah]\n",
    "# joke_index = random.randint(0, 15000)\n",
    "print('index: ', joke_index)\n",
    "print('rating: ', guesses[joke_index])\n",
    "print(data['text'][joke_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's gonna be so good it'll damage your hull. \n"
     ]
    }
   ],
   "source": [
    "print(data['text'][8278])    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
