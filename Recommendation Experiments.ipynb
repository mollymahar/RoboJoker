{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.data import find\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import random\n",
    "\n",
    "# data = pd.read_csv('./all_cc_jokes.csv', sep = ',', index_col = 0, names = ['type', 'link', 'text'])\n",
    "data = pd.read_csv('combined_jokes.csv', sep = ',', index_col = 0)\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "tfidfvectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "tfidf_counts = tfidfvectorizer.fit_transform(data['text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def latent_topic(joke_index, tfidf_counts, tfidfvectorizer, model):\n",
    "    \"\"\"\n",
    "    creates a latent topic for a joke using an existing word2vec model\n",
    "    does so by doing a weighted avg of word2vec \n",
    "    \"\"\"\n",
    "    print('\\rdoing joke {}'.format(joke_index), end='')\n",
    "    scores = tfidf_counts[joke_index]\n",
    "    result = None\n",
    "    for index in scores.indices:\n",
    "        curr_word = tfidfvectorizer.get_feature_names()[index]\n",
    "        try:\n",
    "            if result is None:\n",
    "                result = model[curr_word]\n",
    "            else:\n",
    "                result += model[curr_word]\n",
    "        except:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "def latent_topic2(joke_index, tfidf_counts, tfidfvectorizer, model):\n",
    "    \"\"\"\n",
    "    creates a latent topic for a joke using an existing word2vec model\n",
    "    does so by doing a weighted avg of word2vec \n",
    "    \"\"\"\n",
    "    print('\\rdoing joke {}'.format(joke_index), end='')\n",
    "    scores = tfidf_counts[joke_index]\n",
    "    result = None\n",
    "    for index in scores.indices:\n",
    "        curr_word = tfidfvectorizer.get_feature_names()[index]\n",
    "        try:\n",
    "            if result is None:\n",
    "                result = model[curr_word].reshape((1,300))\n",
    "            else:\n",
    "                result = np.append(result,model[curr_word], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "    if result is None:\n",
    "        return None\n",
    "    mean = np.mean(result, axis=0)\n",
    "    std = np.std(result, axis=0)\n",
    "    return np.append(mean-std, mean+std, axis = 0).reshape((600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating latent topics for jokes file,\n",
      "doing joke 17853\n",
      "done! saved as latent_topics.npy\n"
     ]
    }
   ],
   "source": [
    "generate = False\n",
    "try:\n",
    "    all_latent_topics = np.load('./latent_topics2.npy')\n",
    "    print(\"successfully loaded latent topics for jokes from file\")\n",
    "except:\n",
    "    generate = True\n",
    "\n",
    "if generate:\n",
    "    print(\"generating latent topics for jokes file,\")\n",
    "    all_latent_topics = np.zeros((data.shape[0],300))\n",
    "    for i in range(data.shape[0]):\n",
    "        result = latent_topic(i, tfidf_counts, tfidfvectorizer, model)\n",
    "        if result is not None:\n",
    "            all_latent_topics[i] = result \n",
    "    print(\"\\ndone! saved as latent_topics.npy\")\n",
    "    np.save('./latent_topics.npy', all_latent_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7569\n",
      "A couple celebrates their 30th anniversary by visiting fence against which they first made love. The husband says, \"Come on, for old times' sake.\" The wife agrees and they both undress. Afterwards, the husband says, \"You're even better than you were 30 years ago.\" His wife replies, \"That fence wasn't electrified 30 years ago!\"  \n",
      "skip, identical\n",
      "index 7569\n",
      "actual 3.2504038384e+27\n",
      "ssd 0.0\n",
      "A couple celebrates their 30th anniversary by visiting fence against which they first made love. The husband says, \"Come on, for old times' sake.\" The wife agrees and they both undress. Afterwards, the husband says, \"You're even better than you were 30 years ago.\" His wife replies, \"That fence wasn't electrified 30 years ago!\"  \n",
      "8.72655737864e+14\n",
      "8.72655737864e+14\n",
      "422806400.0\n",
      "28030250240.0\n",
      "0.0\n",
      "7562\n",
      "Two couples go on vacation together. After a week, they are thoroughly bored.   The men decide that maybe life will take on new meaning if they change partners. They all agree that it's an experiment worth trying.  The morning after the switch, one of the husbands says, ''I'm glad we tried this. It was exhilarating. Come on, let's go in the other room and see how the girls got on.''  \n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(index, latent_topics, jokes):\n",
    "    curr_topic = latent_topics[index]\n",
    "    \n",
    "    ssd = np.sum(np.abs(latent_topics - curr_topic), axis=1)\n",
    "    index = 0\n",
    "\n",
    "    while ssd[np.argsort(ssd)[index]] == 0 and index < 8:\n",
    "        print (\"skip, identical\")\n",
    "        print ('index', np.argsort(ssd)[index])\n",
    "        print ('actual', np.sum(np.square(latent_topics[index] - curr_topic)))\n",
    "        print ('ssd', ssd[np.argsort(ssd)[index]])\n",
    "        print (jokes[np.argsort(ssd)[index]])\n",
    "        index += 1\n",
    "    print (np.sum(np.square(latent_topics[np.argsort(ssd)[index]] - curr_topic)))\n",
    "    print(np.sum(np.square(latent_topics - curr_topic), axis=1)[np.argsort(ssd)[index]])\n",
    "    print (ssd[np.argsort(ssd)[index]])\n",
    "    print (ssd[np.argsort(ssd)[index+1]])\n",
    "    print (ssd[np.argsort(ssd)[index-1]])\n",
    "\n",
    "    return np.argsort(ssd)[index]\n",
    "\n",
    "# set to random, or you can set index to a number\n",
    "index=random.randint(0,data.shape[0])\n",
    "# index=13929\n",
    "print(index)\n",
    "print (data['text'][index])\n",
    "similar_index = find_most_similar(index, all_latent_topics, data['text'])\n",
    "print(similar_index)\n",
    "print (data['text'][similar_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  14652\n",
      "rating:  9.0\n",
      "Yo' Mama is so nasty, the sign in the front yard says, \"Beware of Mama.\" \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import linear_model\n",
    "\n",
    "def guess_ratings(indices, ratings, joke_features):\n",
    "    alpha = 9999999999\n",
    "#     lasso =linear_model.LinearRegression()\n",
    "#     lasso = Lasso(alpha=alpha, max_iter = 20000, tol=.1)\n",
    "    lasso = linear_model.Ridge (alpha = .1)\n",
    "#     lasso = ElasticNet(alpha=alpha, l1_ratio=0.7, max_iter=20000)\n",
    "#     lasso\n",
    "\n",
    "    y_pred_lasso = lasso.fit(joke_features[indices], ratings).predict(joke_features)\n",
    "    y_pred_lasso -= np.mean(y_pred_lasso)\n",
    "    y_pred_lasso *= 1.5/np.std(y_pred_lasso)\n",
    "    y_pred_lasso += 2.5\n",
    "    y_pred_lasso = y_pred_lasso.round()\n",
    "    y_pred_lasso[np.where(y_pred_lasso > 5)] = 5\n",
    "    y_pred_lasso[np.where(y_pred_lasso < 1)] = 1\n",
    "    return y_pred_lasso\n",
    "\n",
    "\n",
    "# add a joke's index and then its corresponding rating \n",
    "indices = [9539, 9943, 14327, 14328, 13048, 13058, 8698, 8701, 2578, 2598, 5497, 6235, 13886, 0, 15017, 13882, 10817, 13867, 14196, 13860, 13857, 10764, 13876, 10830, 10820, 13894, 13889, 13864, 13893, 14086, 13871, 13874, 12070, 13861, 13829, 13871, 12806, 12916, 14609, 14506, 14232, 13892, 13896, 13052, 13078, 4479, 10665, 3220, 5178, 4971, 10065, 14513, 4851, 14453, 14406, 14427, 221, 1399, 9912, 294, 9192, 130, 9960, 5156, 14276, 10745, 10175, 14434]\n",
    "ratings = [4.5, 2, 3.3, 4, 5, 2, 3.8, 3, 2, 4, 3.5, 1, 3, 4, 3.5, 3, 0, 1, .5, 0, 2, 1, 0, 4, 4.5, 4.5, 2, 3, 3, 1, 2, 4, 1, 4, 2, 3, 4, 0, 4, 1, 2.1, 1, 2, 0, 0, 0, 5, 3, 4.5, 5, 1, 3.3, 3.4, 3, 3, 1, 0, 0, 0, 3, 2, 3, 1, 3, 2, 2, 2, 5]\n",
    "indices, ratings = np.asarray(indices), np.asarray(ratings)\n",
    "guesses = guess_ratings(indices, ratings, all_latent_topics)\n",
    "\n",
    "blah = -1\n",
    "# uncomment this to get \"good jokes\"\n",
    "# blah = random.randint(-1000, -1)\n",
    "\n",
    "# uncomment this to get \"bad jokes\"\n",
    "# blah = random.randint(0, 1000)\n",
    "\n",
    "# uncomment this to get a completely random joke\n",
    "# blah = random.randint(0, 15000)\n",
    "\n",
    "\n",
    "joke_index = np.argsort(guesses)[blah]\n",
    "while joke_index in indices:\n",
    "    blah -= 1\n",
    "    joke_index = np.argsort(guesses)[blah]\n",
    "# joke_index = random.randint(0, 15000)\n",
    "print('index: ', joke_index)\n",
    "print('rating: ', guesses[joke_index])\n",
    "print(data['text'][joke_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return rating guestimate given existing ratings\n",
    "def guess_ratings(indices, ratings, joke_features):\n",
    "    clf = linear_model.Ridge (alpha = .1)\n",
    "    y_pred = clf.fit(joke_features[indices], ratings).predict(joke_features)\n",
    "    y_pred -= np.mean(y_pred)\n",
    "    y_pred *= 1.5/np.std(y_pred)\n",
    "    y_pred += 2.5\n",
    "    y_pred = y_pred.round()\n",
    "    y_pred[np.where(y_pred > 5)] = 5\n",
    "    y_pred[np.where(y_pred < 1)] = 1\n",
    "    return y_pred\n",
    "\n",
    "# get top five jokes\n",
    "def get_top_five_jokes(indices, ratings, all_jokes, all_latent_topics):\n",
    "#     all_jokes = load_jokes()['text']\n",
    "#     all_latent_topics = load_latent_topics()\n",
    "#     rating_tuples = list(ratings_dict.items())\n",
    "#     indices = [int(rating[0]) for rating in rating_tuples]\n",
    "#     ratings = [int(rating[1]) for rating in rating_tuples]\n",
    "\n",
    "    indices, ratings = np.asarray(indices), np.asarray(ratings)\n",
    "    guesses = guess_ratings(indices, ratings, all_latent_topics)\n",
    "\n",
    "    top_five_idx, top_five_txt, top_five_rating = [], [], []\n",
    "\n",
    "    joke_rankings = np.argsort(guesses)\n",
    "    pos = -1\n",
    "    joke_index = joke_rankings[pos]\n",
    "\n",
    "    while len(top_five_idx) < 5:\n",
    "        while joke_index in indices or joke_index in top_five_idx:\n",
    "            pos -= 1\n",
    "            joke_index = joke_rankings[pos]\n",
    "\n",
    "        top_five_idx += [joke_index]\n",
    "        top_five_txt += [all_jokes[joke_index]]\n",
    "        top_five_rating += [guesses[joke_index]]\n",
    "\n",
    "    print(top_five_idx)\n",
    "    print(top_five_txt)\n",
    "    print(top_five_rating)\n",
    "    return top_five_idx, top_five_txt, top_five_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13866, 15860, 16956, 14070, 14075]\n",
      "['A guy walks into a bar and says to the barman, \"Give me six double vodkas.\"\\n \\n The barman says, \"Wow, you must have had one hell of a day.\"\\n \\n \"Yeah, I just found out my oldest son is gay.\"\\n \\n The next day, the same guy comes into the bar and asks for six more double vodkas. When the bartender asks what\\'s wrong, the man says, \"I just found out that my youngest son is gay, too!\"\\n \\n On the third day, the guy comes into the bar and orders another six double vodkas. The bartender says, \"Jesus! Doesn\\'t anybody in your family like women?\"\\n \\n The man downs the first drink and shakes his head, \"Yeah, my wife!\" ', 'I always knew that I could never be a lawyer because of my inability to pass a bar.', 'Great big polar bear(she says what?) It broke the ice!', 'What do you call a nun who just passed her bar exam? A sister-in-law. ', \"A doctor notices a sidewalk stand that says 'brains for sale.' He   goes over to investigate and sees a sign that says   'Doctor brains $8.00 a pound\\x92 and another sign that   says \\x91Paramedic brains $12.00 a pound, Nurses   brains $30.00 a pound, truck driver $40.00 a pound   and lawyers brains $90.00 a pound.\\x92  \\n So he asks the man behind the cashregister, \\x93how   come his brains are only worth 8.00 and a lawyer's   worth 90.00?\\x94 \\n The man replies, \\x93do you know how   many lawyers it takes to make a pound of brains?\\x94 \"]\n",
      "[5.0, 5.0, 5.0, 5.0, 5.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([13866, 15860, 16956, 14070, 14075],\n",
       " ['A guy walks into a bar and says to the barman, \"Give me six double vodkas.\"\\n \\n The barman says, \"Wow, you must have had one hell of a day.\"\\n \\n \"Yeah, I just found out my oldest son is gay.\"\\n \\n The next day, the same guy comes into the bar and asks for six more double vodkas. When the bartender asks what\\'s wrong, the man says, \"I just found out that my youngest son is gay, too!\"\\n \\n On the third day, the guy comes into the bar and orders another six double vodkas. The bartender says, \"Jesus! Doesn\\'t anybody in your family like women?\"\\n \\n The man downs the first drink and shakes his head, \"Yeah, my wife!\" ',\n",
       "  'I always knew that I could never be a lawyer because of my inability to pass a bar.',\n",
       "  'Great big polar bear(she says what?) It broke the ice!',\n",
       "  'What do you call a nun who just passed her bar exam? A sister-in-law. ',\n",
       "  \"A doctor notices a sidewalk stand that says 'brains for sale.' He   goes over to investigate and sees a sign that says   'Doctor brains $8.00 a pound\\x92 and another sign that   says \\x91Paramedic brains $12.00 a pound, Nurses   brains $30.00 a pound, truck driver $40.00 a pound   and lawyers brains $90.00 a pound.\\x92  \\n So he asks the man behind the cashregister, \\x93how   come his brains are only worth 8.00 and a lawyer's   worth 90.00?\\x94 \\n The man replies, \\x93do you know how   many lawyers it takes to make a pound of brains?\\x94 \"],\n",
       " [5.0, 5.0, 5.0, 5.0, 5.0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [9539, 9943, 14327, 14328, 13048, 13058, 8698, 8701, 2578, 2598, 5497, 6235, 13886, 0, 15017, 13882, 10817, 13867, 14196, 13860, 13857, 10764, 13876, 10830, 10820, 13894, 13889, 13864, 13893, 14086, 13871, 13874, 12070, 13861, 13829, 13871, 12806, 12916, 14609, 14506, 14232, 13892, 13896, 13052, 13078, 4479, 10665, 3220, 5178, 4971, 10065, 14513, 4851, 14453, 14406, 14427, 221, 1399, 9912, 294, 9192, 130, 9960, 5156, 14276, 10745, 10175, 14434]\n",
    "ratings = [4.5, 2, 3.3, 4, 5, 2, 3.8, 3, 2, 4, 3.5, 1, 3, 4, 3.5, 3, 0, 1, .5, 0, 2, 1, 0, 4, 4.5, 4.5, 2, 3, 3, 1, 2, 4, 1, 4, 2, 3, 4, 0, 4, 1, 2.1, 1, 2, 0, 0, 0, 5, 3, 4.5, 5, 1, 3.3, 3.4, 3, 3, 1, 0, 0, 0, 3, 2, 3, 1, 3, 2, 2, 2, 5]\n",
    "all_jokes = data['text']\n",
    "get_top_five_jokes(indices, ratings, all_jokes, all_latent_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
